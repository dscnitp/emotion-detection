{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9KuIpa_p3gj"
      },
      "source": [
        "Ipython notebook for emotion detection from text using LSTM (Long short term memory.\n",
        "The first step is the data preprocessing of the corwdflower dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LL4vQYYsX4n"
      },
      "source": [
        "#Importing the required libraries\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,LSTM,Input,Bidirectional\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNjDjl_Aq4L5"
      },
      "source": [
        "Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2zqjb2hp0Yk",
        "outputId": "70b32391-407b-432b-f7c1-d9e0673a4b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbYqXZFdrf5r"
      },
      "source": [
        "Importing the datasets from the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ-dmbLkrNpL",
        "outputId": "694e54ea-b073-45af-8df7-d35bf5403c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/text_emotion.csv')\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PU9yiknruS4"
      },
      "source": [
        "Visualising the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaudXnLVrs54",
        "outputId": "7833955a-1dba-4f5e-9788-190fba86ae38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     tweet_id  ...                                            content\n",
            "0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
            "1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2  1956967696  ...                Funeral ceremony...gloomy friday...\n",
            "3  1956967789  ...               wants to hang out with friends SOON!\n",
            "4  1956968416  ...  @dannycastillo We want to trade with someone w...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WIbyeEMsCyh"
      },
      "source": [
        "The tweet_id and author name are of no use to us. So drop these columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S-EIJtXr6SV",
        "outputId": "29e174c9-6394-45a0-fb0c-7a7234b495d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df=df.drop(['tweet_id','author'], axis = 1) \n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    sentiment                                            content\n",
            "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
            "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2     sadness                Funeral ceremony...gloomy friday...\n",
            "3  enthusiasm               wants to hang out with friends SOON!\n",
            "4     neutral  @dannycastillo We want to trade with someone w...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHoXhPXsuK_"
      },
      "source": [
        "Now, we have all the required columns. Now, using NLP for text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw-NYVDqYbWu"
      },
      "source": [
        "# Removing URL from the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('http\\S+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4c-HLYV6dq3"
      },
      "source": [
        "# Removing words which starts with '@'(mentioning a user or page) in the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('@\\w+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGpnHm96g2i"
      },
      "source": [
        "# Removing words which starts with '#'(representing any trend) in the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('#\\w+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_lXaSOEFIfP"
      },
      "source": [
        "# phrases cleaning & punctuation removal\n",
        "\n",
        "import re\n",
        "def sentence_cleaning(sentence):\n",
        "    sentence=re.sub('\\'d',' would',sentence)\n",
        "    sentence=re.sub('\\'ll',' will',sentence)\n",
        "    sentence=re.sub('\\'ve',' have',sentence)\n",
        "    sentence=re.sub('\\'s',' is',sentence)\n",
        "    sentence=re.sub('n\\'t',' not',sentence)\n",
        "    sentence=re.sub(\"won't\",' will not',sentence)\n",
        "    sentence=re.sub(\"can't\",' cannot',sentence)\n",
        "    sentence=re.sub(\"ain't\",' am not',sentence)\n",
        "    sentence=re.sub('\\W',' ',sentence) #'\\W' to remove all non-alphanumeric characters(punctuations)\n",
        "    sentence=sentence.lower()\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mErf0MocFu1y",
        "outputId": "25c95ab8-ebee-45b5-ce5f-bb1f72448521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df['content'] = df['content'].apply(sentence_cleaning)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>funeral ceremony   gloomy friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>we want to trade with someone who has houston...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty   i know  i was listenin to bad habit earlier a...\n",
              "1     sadness  layin n bed with a headache  ughhhh   waitin o...\n",
              "2     sadness                funeral ceremony   gloomy friday   \n",
              "3  enthusiasm               wants to hang out with friends soon \n",
              "4     neutral   we want to trade with someone who has houston..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eetpKWmAZTd5"
      },
      "source": [
        " Removing stopwords from the tweets and reducing each word to its lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRZEKhrNZpAj",
        "outputId": "261c6368-fddc-4f8f-a833-c6f536a17a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7hJ_IpAGRsL"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2mZtFzZaWM"
      },
      "source": [
        "def stop_word_removal(words):\n",
        "    cleaned_line=[]\n",
        "    for i in words:\n",
        "        if i not in stopwords.words('english'):\n",
        "            cleaned_line.append(i)\n",
        "    return cleaned_line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7yuTi9b2kc",
        "outputId": "f3ea3523-7c62-42b7-fbaf-cc85cb7ba2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_phrase = []\n",
        "\n",
        "for x in tqdm(df['content']):    \n",
        "    word_tokens = word_tokenize(x)\n",
        "    \n",
        "    # lemmatizing each word in the list\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma = [lemmatizer.lemmatize(i) for  i in word_tokens]\n",
        "    \n",
        "    # stop word removal\n",
        "    cleaned_text = stop_word_removal(lemma)\n",
        "    \n",
        "    text = \" \".join(cleaned_text)\n",
        "    n_phrase.append(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [01:08<00:00, 582.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYw1O5cdLrw",
        "outputId": "bff8a862-44c5-4532-d7e3-c871bfcd9785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.loc[:,'content'] = n_phrase\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>know wa listenin bad habit earlier started fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>layin n bed headache ughhhh waitin call</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>funeral ceremony gloomy friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>want hang friend soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>want trade someone ha houston ticket one</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  know wa listenin bad habit earlier started fre...\n",
              "1     sadness            layin n bed headache ughhhh waitin call\n",
              "2     sadness                     funeral ceremony gloomy friday\n",
              "3  enthusiasm                              want hang friend soon\n",
              "4     neutral           want trade someone ha houston ticket one"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh0o-v-GdqbT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAYjqvjad-6O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}