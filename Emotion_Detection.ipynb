{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9KuIpa_p3gj"
      },
      "source": [
        "Ipython notebook for emotion detection from text using LSTM (Long short term memory.\n",
        "The first step is the data preprocessing of the corwdflower dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LL4vQYYsX4n"
      },
      "source": [
        "#Importing the required libraries\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,LSTM,Input,Bidirectional\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNjDjl_Aq4L5"
      },
      "source": [
        "Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2zqjb2hp0Yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b9732-1aef-4084-c2fc-45dff4ca57d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbYqXZFdrf5r"
      },
      "source": [
        "Importing the datasets from the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ-dmbLkrNpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9369d62a-c3b7-4240-8f84-46c8abd1e599"
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/text_emotion.csv')\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PU9yiknruS4"
      },
      "source": [
        "Visualising the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaudXnLVrs54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3723219b-9175-4ab1-c78b-6be0fb17d7c2"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     tweet_id  ...                                            content\n",
            "0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
            "1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2  1956967696  ...                Funeral ceremony...gloomy friday...\n",
            "3  1956967789  ...               wants to hang out with friends SOON!\n",
            "4  1956968416  ...  @dannycastillo We want to trade with someone w...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WIbyeEMsCyh"
      },
      "source": [
        "The tweet_id and author name are of no use to us. So drop these columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S-EIJtXr6SV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edceeab6-d233-46b8-b600-55d2bb4791d4"
      },
      "source": [
        "df=df.drop(['tweet_id','author'], axis = 1) \n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    sentiment                                            content\n",
            "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
            "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2     sadness                Funeral ceremony...gloomy friday...\n",
            "3  enthusiasm               wants to hang out with friends SOON!\n",
            "4     neutral  @dannycastillo We want to trade with someone w...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHoXhPXsuK_"
      },
      "source": [
        "Now, we have all the required columns. Now, using NLP for text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QW0jSUL2960"
      },
      "source": [
        "# Removing URL from the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('http\\S+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4c-HLYV6dq3"
      },
      "source": [
        "# Removing words which starts with '@'(mentioning a user or page) in the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('@\\w+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGpnHm96g2i"
      },
      "source": [
        "# Removing words which starts with '#'(representing any trend) in the tweets\n",
        "\n",
        "df['content'] = df['content'].str.replace(re.compile('#\\w+'),'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_lXaSOEFIfP"
      },
      "source": [
        "# phrases cleaning & punctuation removal\n",
        "\n",
        "import re\n",
        "def sentence_cleaning(sentence):\n",
        "    sentence=re.sub('\\'d',' would',sentence)\n",
        "    sentence=re.sub('\\'ll',' will',sentence)\n",
        "    sentence=re.sub('\\'ve',' have',sentence)\n",
        "    sentence=re.sub('\\'s',' is',sentence)\n",
        "    sentence=re.sub('n\\'t',' not',sentence)\n",
        "    sentence=re.sub(\"won't\",' will not',sentence)\n",
        "    sentence=re.sub(\"can't\",' cannot',sentence)\n",
        "    sentence=re.sub(\"ain't\",' am not',sentence)\n",
        "    sentence=re.sub('\\W',' ',sentence) #'\\W' to remove all non-alphanumeric characters(punctuations)\n",
        "    sentence=sentence.lower()\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mErf0MocFu1y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1b24b67f-6f6f-4080-e0c0-2f5d9656b875"
      },
      "source": [
        "df['content'] = df['content'].apply(sentence_cleaning)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>funeral ceremony   gloomy friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>we want to trade with someone who has houston...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty   i know  i was listenin to bad habit earlier a...\n",
              "1     sadness  layin n bed with a headache  ughhhh   waitin o...\n",
              "2     sadness                funeral ceremony   gloomy friday   \n",
              "3  enthusiasm               wants to hang out with friends soon \n",
              "4     neutral   we want to trade with someone who has houston..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eetpKWmAZTd5"
      },
      "source": [
        " Removing stopwords from the tweets and reducing each word to its lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRZEKhrNZpAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb5e59c-ef01-46b5-d0cb-7039334d8317"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7hJ_IpAGRsL"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2mZtFzZaWM"
      },
      "source": [
        "def stop_word_removal(words):\n",
        "    cleaned_line=[]\n",
        "    for i in words:\n",
        "        if i not in stopwords.words('english'):\n",
        "            cleaned_line.append(i)\n",
        "    return cleaned_line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7yuTi9b2kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4354e118-3486-4468-b2ab-b2d100aca106"
      },
      "source": [
        "n_phrase = []\n",
        "\n",
        "for x in tqdm(df['content']):    \n",
        "    word_tokens = word_tokenize(x)\n",
        "    \n",
        "    # lemmatizing each word in the list\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma = [lemmatizer.lemmatize(i) for  i in word_tokens]\n",
        "    \n",
        "    # stop word removal\n",
        "    cleaned_text = stop_word_removal(lemma)\n",
        "    \n",
        "    text = \" \".join(cleaned_text)\n",
        "    n_phrase.append(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:58<00:00, 687.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYw1O5cdLrw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "dcd3373b-98bc-4c95-b8e3-5f7f40237489"
      },
      "source": [
        "df.loc[:,'content'] = n_phrase\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>know wa listenin bad habit earlier started fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>layin n bed headache ughhhh waitin call</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>funeral ceremony gloomy friday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>want hang friend soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>want trade someone ha houston ticket one</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  know wa listenin bad habit earlier started fre...\n",
              "1     sadness            layin n bed headache ughhhh waitin call\n",
              "2     sadness                     funeral ceremony gloomy friday\n",
              "3  enthusiasm                              want hang friend soon\n",
              "4     neutral           want trade someone ha houston ticket one"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o_sFNqYM86"
      },
      "source": [
        "## Text Vectorization\n",
        "\n",
        "creating word to indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh0o-v-GdqbT"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAYjqvjad-6O"
      },
      "source": [
        "texts = df['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js4GIoeKYl5L"
      },
      "source": [
        "# Indexing words\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_indices = tokenizer.texts_to_sequences(texts)\n",
        "word_map = tokenizer.word_index\n",
        "\n",
        "# print(word_indices)\n",
        "# print(word_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ArK1wDZS4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d913c32-fbdd-477c-b501-e1378d9a9f37"
      },
      "source": [
        "seq_length = max(word_indices,key = lambda x: len(x))\n",
        "print(len(seq_length))\n",
        "\n",
        "# maximum length of any sequence of words be 50\n",
        "seq_length = 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RYiGLUqaCok"
      },
      "source": [
        "# padding words\n",
        "\n",
        "X_data = pad_sequences(word_indices,padding='pre',maxlen=seq_length)\n",
        "\n",
        "# print(X_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ftm10x9-hZY"
      },
      "source": [
        "# Preparing the output data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPGJ8WQW_din",
        "outputId": "9fde19be-da81-41c8-dc97-d919085628f9"
      },
      "source": [
        "y = df['sentiment'].unique()\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
            " 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wfzHJGc6lRX",
        "outputId": "cb306366-6de5-4409-cfa9-67ee42cf7c33"
      },
      "source": [
        "y_map = {} \n",
        "for i in range(len(y)):\n",
        "  y_map[y[i]] = i\n",
        "\n",
        "print(y_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'empty': 0, 'sadness': 1, 'enthusiasm': 2, 'neutral': 3, 'worry': 4, 'surprise': 5, 'love': 6, 'fun': 7, 'hate': 8, 'happiness': 9, 'boredom': 10, 'relief': 11, 'anger': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh_6L4hTJYY2"
      },
      "source": [
        "df.sentiment = df.sentiment.map(y_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuonxmRhAGFt"
      },
      "source": [
        "# Generating one hot encoded data for the sentiment from the above labeled data\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMch--eXKXtR",
        "outputId": "6183fcfc-2f6e-4da7-97b8-e86c550f7b2c"
      },
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "res = enc.fit_transform(df[['sentiment']]).toarray()\n",
        "\n",
        "print(res.shape)\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzFThC8nEqT6",
        "outputId": "32cd5a7b-69e0-49b9-d311-524c0bfa58f1"
      },
      "source": [
        "Y_data = res\n",
        "print(Y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwD_Jhe8bmIx"
      },
      "source": [
        "## Loading Pre-trained GloVe\n",
        "\n",
        "We are using the 50D GloVe vectors hence each word is represented in a 50D embbedding space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6SzEXoFaxul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1678c39a-be81-42b1-b5aa-6f4c152f983c"
      },
      "source": [
        "# to store GloVe vectors\n",
        "embeddings_dict = {}\n",
        "\n",
        "with open(\"/content/drive/My Drive/glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "f.close()\n",
        "\n",
        "print('====successfully loaded======')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====successfully loaded======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwMWZy0RcZUS"
      },
      "source": [
        "Building Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMNJIYz_cNi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc18e99f-0a00-49dd-a681-837a7da49d9e"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_map) + 1,50))\n",
        "\n",
        "\n",
        "for word, idx in word_map.items():\n",
        "    vector = embeddings_dict.get(word)\n",
        "    if vector is not None:\n",
        "        embedding_matrix[idx] = vector\n",
        "\n",
        "print(\"matrix shape : \",embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matrix shape :  (27429, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pd9Kjd_dZdd"
      },
      "source": [
        "embedding_layer = Embedding(len(word_map) + 1,50, weights=[embedding_matrix],input_length=seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ioz2bnCufWFF"
      },
      "source": [
        "# implementing the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_W0kjKn1euu"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BslvyVzQkbJH"
      },
      "source": [
        "model= Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(units = 100,return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 520,activation='relu'))\n",
        "model.add(Dense(units = 260,activation='relu'))\n",
        "model.add(Dense(len(y_data),activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NokEM3B-CpCM",
        "outputId": "96c3b766-ea4b-4d15-e657-30da4e9ecdc8"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            1371450   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 520)               2600520   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 260)               135460    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 13)                3393      \n",
            "=================================================================\n",
            "Total params: 4,171,223\n",
            "Trainable params: 4,171,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdn7rSeC3ye"
      },
      "source": [
        "# splitting the dataset for training \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SgsFF-gEg7D"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(X_data,Y_data,train_size=0.8,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTr6CmRIFOvV",
        "outputId": "237193b6-595e-4be3-e4fd-f7e751227986"
      },
      "source": [
        "print(\"X Training data: \",x_train.shape)\n",
        "print(\"Y training data: \",y_train.shape)\n",
        "print(\"X test data\",x_test.shape)\n",
        "print(\"Y test data\",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Training data:  (32000, 50)\n",
            "Y training data:  (32000, 13)\n",
            "X test data (8000, 50)\n",
            "Y test data (8000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V1Wva7VTCfN",
        "outputId": "84d2d04c-2f54-4b68-8e4a-5f166baffae8"
      },
      "source": [
        "x_val = x_train[:8000,:]\n",
        "y_val = y_train[:8000,:]\n",
        "x_train = x_train[8000:,:]\n",
        "y_train = y_train[8000:,:]\n",
        "print(\"validation input data:\", x_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation input data: (8000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duZl8WRJFfXX",
        "outputId": "08dc0131-86b6-40d7-b75c-00bef67e31d7"
      },
      "source": [
        "model.fit(x_train,y_train,validation_data = (x_val,y_val),batch_size = 128,epochs = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "188/188 [==============================] - 2s 10ms/step - loss: 0.7399 - accuracy: 0.8565 - val_loss: 0.7952 - val_accuracy: 0.8264\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4912 - accuracy: 0.8926 - val_loss: 0.7445 - val_accuracy: 0.8411\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3668 - accuracy: 0.9154 - val_loss: 0.7315 - val_accuracy: 0.8459\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2898 - accuracy: 0.9305 - val_loss: 0.7595 - val_accuracy: 0.8365\n",
            "Epoch 5/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2423 - accuracy: 0.9420 - val_loss: 0.7483 - val_accuracy: 0.8384\n",
            "Epoch 6/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2131 - accuracy: 0.9488 - val_loss: 0.7540 - val_accuracy: 0.8378\n",
            "Epoch 7/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1932 - accuracy: 0.9531 - val_loss: 0.7618 - val_accuracy: 0.8356\n",
            "Epoch 8/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1785 - accuracy: 0.9559 - val_loss: 0.7746 - val_accuracy: 0.8328\n",
            "Epoch 9/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1664 - accuracy: 0.9598 - val_loss: 0.7833 - val_accuracy: 0.8331\n",
            "Epoch 10/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1583 - accuracy: 0.9617 - val_loss: 0.7879 - val_accuracy: 0.8307\n",
            "Epoch 11/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1513 - accuracy: 0.9634 - val_loss: 0.8032 - val_accuracy: 0.8311\n",
            "Epoch 12/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1453 - accuracy: 0.9643 - val_loss: 0.8005 - val_accuracy: 0.8288\n",
            "Epoch 13/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1404 - accuracy: 0.9658 - val_loss: 0.8094 - val_accuracy: 0.8273\n",
            "Epoch 14/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1355 - accuracy: 0.9668 - val_loss: 0.8110 - val_accuracy: 0.8269\n",
            "Epoch 15/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1321 - accuracy: 0.9673 - val_loss: 0.8192 - val_accuracy: 0.8251\n",
            "Epoch 16/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1291 - accuracy: 0.9671 - val_loss: 0.8346 - val_accuracy: 0.8201\n",
            "Epoch 17/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1261 - accuracy: 0.9682 - val_loss: 0.8333 - val_accuracy: 0.8202\n",
            "Epoch 18/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1230 - accuracy: 0.9693 - val_loss: 0.8375 - val_accuracy: 0.8194\n",
            "Epoch 19/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1207 - accuracy: 0.9696 - val_loss: 0.8577 - val_accuracy: 0.8117\n",
            "Epoch 20/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1194 - accuracy: 0.9697 - val_loss: 0.8467 - val_accuracy: 0.8174\n",
            "Epoch 21/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1176 - accuracy: 0.9707 - val_loss: 0.8545 - val_accuracy: 0.8127\n",
            "Epoch 22/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1144 - accuracy: 0.9708 - val_loss: 0.8591 - val_accuracy: 0.8124\n",
            "Epoch 23/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1126 - accuracy: 0.9708 - val_loss: 0.8642 - val_accuracy: 0.8158\n",
            "Epoch 24/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1129 - accuracy: 0.9712 - val_loss: 0.8754 - val_accuracy: 0.8105\n",
            "Epoch 25/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1106 - accuracy: 0.9708 - val_loss: 0.9328 - val_accuracy: 0.7954\n",
            "Epoch 26/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1095 - accuracy: 0.9711 - val_loss: 0.8772 - val_accuracy: 0.8117\n",
            "Epoch 27/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1080 - accuracy: 0.9716 - val_loss: 0.8835 - val_accuracy: 0.8067\n",
            "Epoch 28/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1061 - accuracy: 0.9715 - val_loss: 0.8893 - val_accuracy: 0.8066\n",
            "Epoch 29/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1051 - accuracy: 0.9721 - val_loss: 0.8883 - val_accuracy: 0.8052\n",
            "Epoch 30/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1037 - accuracy: 0.9724 - val_loss: 0.9026 - val_accuracy: 0.8058\n",
            "Epoch 31/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1040 - accuracy: 0.9720 - val_loss: 0.9073 - val_accuracy: 0.8030\n",
            "Epoch 32/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1025 - accuracy: 0.9722 - val_loss: 0.9106 - val_accuracy: 0.8043\n",
            "Epoch 33/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1011 - accuracy: 0.9730 - val_loss: 0.9092 - val_accuracy: 0.8045\n",
            "Epoch 34/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1010 - accuracy: 0.9722 - val_loss: 0.9158 - val_accuracy: 0.8052\n",
            "Epoch 35/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0994 - accuracy: 0.9728 - val_loss: 0.9194 - val_accuracy: 0.8012\n",
            "Epoch 36/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0980 - accuracy: 0.9722 - val_loss: 0.9292 - val_accuracy: 0.8008\n",
            "Epoch 37/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0980 - accuracy: 0.9730 - val_loss: 0.9328 - val_accuracy: 0.7984\n",
            "Epoch 38/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0969 - accuracy: 0.9725 - val_loss: 0.9415 - val_accuracy: 0.7997\n",
            "Epoch 39/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0965 - accuracy: 0.9735 - val_loss: 0.9349 - val_accuracy: 0.8008\n",
            "Epoch 40/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0951 - accuracy: 0.9733 - val_loss: 0.9368 - val_accuracy: 0.7969\n",
            "Epoch 41/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0957 - accuracy: 0.9725 - val_loss: 0.9400 - val_accuracy: 0.7955\n",
            "Epoch 42/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0944 - accuracy: 0.9727 - val_loss: 0.9450 - val_accuracy: 0.8006\n",
            "Epoch 43/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0935 - accuracy: 0.9734 - val_loss: 0.9570 - val_accuracy: 0.7939\n",
            "Epoch 44/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0935 - accuracy: 0.9732 - val_loss: 0.9551 - val_accuracy: 0.7946\n",
            "Epoch 45/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0939 - accuracy: 0.9725 - val_loss: 0.9551 - val_accuracy: 0.7952\n",
            "Epoch 46/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0917 - accuracy: 0.9746 - val_loss: 0.9608 - val_accuracy: 0.7945\n",
            "Epoch 47/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0915 - accuracy: 0.9735 - val_loss: 0.9804 - val_accuracy: 0.7891\n",
            "Epoch 48/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0916 - accuracy: 0.9734 - val_loss: 1.0017 - val_accuracy: 0.7859\n",
            "Epoch 49/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0909 - accuracy: 0.9735 - val_loss: 0.9722 - val_accuracy: 0.7926\n",
            "Epoch 50/50\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0903 - accuracy: 0.9740 - val_loss: 0.9698 - val_accuracy: 0.7910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f38c2239d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWketdUyOc52",
        "outputId": "bf3d0e80-f458-483d-a8cc-a1be4ce09ad4"
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7926250100135803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsW8QKIoQkuT"
      },
      "source": [
        "# Preparing input to predict\n",
        "\n",
        "def prepare(s):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([s])\n",
        "  word_indices = tokenizer.texts_to_sequences([s])\n",
        "  X = pad_sequences(word_indices,padding='pre',maxlen=seq_length)\n",
        "  arr = model.predict(X)\n",
        "  return arr.argmax()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPPARIusWNNL"
      },
      "source": [
        "s = input()\n",
        "out = prepare(s)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aKPUxu7X8Xg",
        "outputId": "f0409509-b84e-4950-d8bb-39658f4cac26"
      },
      "source": [
        "fuy_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anger': 12,\n",
              " 'boredom': 10,\n",
              " 'empty': 0,\n",
              " 'enthusiasm': 2,\n",
              " 'fun': 7,\n",
              " 'happiness': 9,\n",
              " 'hate': 8,\n",
              " 'love': 6,\n",
              " 'neutral': 3,\n",
              " 'relief': 11,\n",
              " 'sadness': 1,\n",
              " 'surprise': 5,\n",
              " 'worry': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft-Vjrd7ZiOT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}