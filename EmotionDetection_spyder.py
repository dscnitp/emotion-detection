# -*- coding: utf-8 -*-
"""SENN_isear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJIb0vfM7RdIA1lXQHfrMScY0544y9As


import keras
import numpy as np
from tensorflow.keras.models import Sequential,Model
from keras.layers import Dense,Dropout,LSTM,Input,Bidirectional
from sklearn.model_selection import cross_val_score 
from nltk.tokenize import word_tokenize,sent_tokenize
from ast import literal_eval
import nltk
nltk.download('punkt')
"""
import keras
import numpy as np
from keras.models import Sequential,Model
from keras.layers import Dense,Dropout,LSTM,Input,Bidirectional
from sklearn.model_selection import cross_val_score 
from nltk.tokenize import word_tokenize,sent_tokenize
from ast import literal_eval
import nltk
nltk.download('punkt')
from keras.layers import *
from sklearn.model_selection import cross_val_score

"""Remove the rows which contains values ['No resposne']"""

import pandas as pd
df=pd.read_csv('isear.csv',header=None)
df.drop(df[df[1] == '[ No response.]'].index, inplace = True)
index_names = df[ df[0] == 'shame' ].index 
df.drop(index_names, inplace = True)
index_names = df[ df[0] == 'disgust' ].index 
df.drop(index_names, inplace = True)
index_names = df[ df[0] == 'guilt' ].index 
df.drop(index_names, inplace = True)
df.reset_index(inplace = True) 
feel_arr=df[1]
feel_arr=[word_tokenize(sent) for sent in feel_arr]
def padd(arr):
    for i in range(100-len(arr)):
        arr.append('<pad>')
    return arr[:100]
for i in range(len(feel_arr)):
  feel_arr[i]=padd(feel_arr[i])
vocab_f=open('glove.6B.50d.txt', encoding="utf-8")
word_to_emb={}
for line in vocab_f:
  word_to_emb[line.split()[0]]=[float(i) for i in line.split()[1:]] 
word_to_emb['<pad>']=[0]*50
print(len(word_to_emb['happy']))
print(word_to_emb['happy'])
embedded_feel_arr=[] 
for each_sentence in feel_arr:
    embedded_feel_arr.append([])
    for word in each_sentence:
        if word.lower() in word_to_emb:
            embedded_feel_arr[-1].append(word_to_emb[word.lower()])
        else:
            embedded_feel_arr[-1].append([0]*50)
X=np.array(embedded_feel_arr)
Y= df[0].unique()
y_map = {} 
for i in range(len(Y)):
  y_map[Y[i]] = i
print(y_map)
df[0] = df[0].map(y_map)
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
res = enc.fit_transform(np.array(df[0]).reshape(-1,1)).toarray()
Y= res
from keras.layers import Embedding
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)
def model1(X,Y,input_size1,input_size2,output_size):
  m=Sequential()
  m.add(Bidirectional(LSTM(100,input_shape=(input_size1,input_size2))))
  m.add(Dropout(0.5))
  m.add(Dense(output_size,activation='softmax'))
  m.compile('Adam','categorical_crossentropy',['accuracy'])
  m.fit(X,Y,epochs=32, batch_size=128)
  return m
bilstmModel=model1(X_train,Y_train,100,50,4)

import joblib
filename='final_model.sav'
joblib.dump(bilstmModel,filename);
bilstmModel.evaluate(X_test,Y_test)
"""
y_inverse_map={}
for i in y_map:
  y_inverse_map[y_map[i]]=i

def prepare(s):
  s=word_tokenize(s)
  k=padd(s)
  embedded_arr=[]
  embedded_arr.append([])
  for word in k:
        if word.lower() in word_to_emb:
            embedded_arr[-1].append(word_to_emb[word.lower()])
        else:
            embedded_arr[-1].append([0]*50)
  x_input=np.array(embedded_arr)
  #print(np.shape(x_input))
  arr = bilstmModel.predict(x_input)
  return arr.argmax()

s=input()
emotion=prepare(s)
print(y_inverse_map[emotion])
"""
